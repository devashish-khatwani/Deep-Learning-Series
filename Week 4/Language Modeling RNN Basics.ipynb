{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "import urllib\n",
    "from urllib.parse import urljoin\n",
    "import gzip\n",
    "import json\n",
    "from os import makedirs\n",
    "from os.path import join, isdir, exists\n",
    "import time\n",
    "import numpy as np\n",
    "from random import randint\n",
    "from hashlib import sha1\n",
    "import pandas as pd\n",
    "\n",
    "AZLYRICS_URL = 'http://www.azlyrics.com/t/taylorswift.html'\n",
    "CACHE_DIR = 'html/'\n",
    "data = pd.DataFrame(columns = ['lyrics'])\n",
    "\n",
    "urlobject = urllib.request.urlopen(AZLYRICS_URL)\n",
    "time.sleep(randint(5, 30))\n",
    "print('Read %s' % AZLYRICS_URL)\n",
    "full_html = urlobject.read()\n",
    "soup = bs4.BeautifulSoup(full_html, \"html.parser\")\n",
    "\n",
    "lyrics_urls = []\n",
    "for item in soup.find_all('a',href = True):\n",
    "    if '/lyrics/' in item['href']:\n",
    "        lyrics_urls.append(('https://www.azlyrics.com'+item['href'][2:]))\n",
    "\n",
    "data = []\n",
    "for link in lyrics_urls:\n",
    "    try:          \n",
    "        lyrics_urlobject = urllib.request.urlopen(link)\n",
    "        full_html = lyrics_urlobject.read()\n",
    "        lyrics_soup = bs4.BeautifulSoup(full_html,'html.parser')\n",
    "        lyrics = str(lyrics_soup)\n",
    "        # lyrics lies between up_partition and down_partition\n",
    "        up_partition = '<!-- Usage of azlyrics.com content by any third-party lyrics provider is prohibited by our licensing agreement. Sorry about that. -->'\n",
    "        down_partition = '<!-- MxM banner -->'\n",
    "        lyrics = lyrics.split(up_partition)[1]\n",
    "        lyrics = lyrics.split(down_partition)[0]\n",
    "        lyrics = lyrics.replace('<br>','').replace('</br>','').replace('</div>','').replace('<br/>','').strip()\n",
    "        data.append(lyrics)\n",
    "        time.sleep(randint(5, 30))\n",
    "        print(len(data))\n",
    "    except:\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"taylor_swift.txt\", \"wb\") as f:\n",
    "    pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"taylor_swift.txt\", \"rb\") as f:\n",
    "    mylist = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\khatwd2\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function #Python 2/3 compatibility for print statements\n",
    "import pandas\n",
    "pandas.set_option('display.max_colwidth', 300) #widen pandas rows display\n",
    "from numpy import array\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Input, Dense, TimeDistributed\n",
    "from keras.layers import Dense\n",
    "from keras.layers import GRU,LSTM\n",
    "from keras.layers import Embedding     \n",
    "from keras.models import Sequential\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "data = pandas.read_csv('data/example_train_stories.csv', encoding='utf-8')[:100]\n",
    "#data[:10]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3516\n",
      "Total Sequences: 67912\n",
      "Max Sequence Length: 754\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "t = Tokenizer()\n",
    "#docs = data['Story'].tolist()\n",
    "docs = copy.deepcopy(mylist)\n",
    "#max_length = max([len(sent.split()) for sent in docs])\n",
    "max_length = 10\n",
    "t.fit_on_texts(docs)\n",
    "vocab_size = len(t.word_index) + 1\n",
    "print(vocab_size)\n",
    "encoded_docs = t.texts_to_sequences(docs)\n",
    "sequences = list()\n",
    "for doc in encoded_docs:\n",
    "    for i in range(1, len(doc)):\n",
    "        if i <= max_length:\n",
    "            sequence = doc[:i+1]\n",
    "        else:\n",
    "            sequence = doc[i-max_length:i]\n",
    "            \n",
    "        sequences.append(sequence)\n",
    "print('Total Sequences: %d' % len(sequences))\n",
    "#max_length = max([len(seq) for seq in sequences])\n",
    "sequences = pad_sequences(sequences, maxlen=max_length, padding='pre')\n",
    "print('Max Sequence Length: %d' % max_length)\n",
    "#print(t.word_counts)\n",
    "#print(t.document_count)\n",
    "#print(t.word_index)\n",
    "#print(t.word_docs)\n",
    "#print(encoded_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into input and output elements\n",
    "sequences = array(sequences)\n",
    "X, y = sequences[:,:-1],sequences[:,-1]\n",
    "y = to_categorical(y, num_classes=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 100, input_length=max_length-1))\n",
    "model.add(LSTM(50))\n",
    "#model.add(LSTM(500,return_sequences=True))\n",
    "#model.add(LSTM(500))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile network\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit network\n",
    "model.fit(X, y, epochs=10, verbose=2, batch_size = 10)\n",
    "model.save_weights(\"taylor_swift_model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a sequence from a language model\n",
    "def generate_seq(model, tokenizer, max_length, seed_text, n_words):\n",
    "\tin_text = seed_text\n",
    "\t# generate a fixed number of words\n",
    "\tfor _ in range(n_words):\n",
    "\t\t# encode the text as integer\n",
    "\t\tencoded = tokenizer.texts_to_sequences([in_text])[0]\n",
    "\t\t# pre-pad sequences to a fixed length\n",
    "\t\tencoded = pad_sequences([encoded], maxlen=max_length, padding='pre')\n",
    "\t\t# predict probabilities for each word\n",
    "\t\tyhat = model.predict_classes(encoded, verbose=0)\n",
    "\t\t# map predicted word index to word\n",
    "\t\tout_word = ''\n",
    "\t\tfor word, index in tokenizer.word_index.items():\n",
    "\t\t\tif index == yhat:\n",
    "\t\t\t\tout_word = word\n",
    "\t\t\t\tbreak\n",
    "\t\t# append to input\n",
    "\t\tin_text += ' ' + out_word\n",
    "\treturn in_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model.load_weights(\"taylor_swift_model.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    "generate_seq(model,t,max_length,'look what you made me do',10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
