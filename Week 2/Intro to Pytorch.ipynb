{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A PyTorch Tensor is conceptually identical to a numpy array: a Tensor is an n-dimensional array, and PyTorch provides many functions for operating on these Tensors. Like numpy arrays, PyTorch Tensors do not know anything about deep learning or computational graphs or gradients; they are a generic tool for scientific computing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import basic libraries\n",
    "import torch \n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = torch.Tensor([1])\n",
    "print(a)\n",
    "\n",
    "b=torch.FloatTensor([[1, 2, 3], [4, 5, 6]])\n",
    "print(b)\n",
    "print(b[0][2])\n",
    "\n",
    "c=torch.IntTensor(2, 4).zero_()\n",
    "print(c)\n",
    "c.fill_(8)\n",
    "print(c)\n",
    "\n",
    "d=torch.Tensor(3, 3).uniform_(0, 1)\n",
    "print(d)\n",
    "\n",
    "e=torch.Tensor(3, 3).exponential_()\n",
    "print(e)\n",
    "\n",
    "f=torch.ones(3, 3)\n",
    "print(f)\n",
    "\n",
    "g_np = np.arange(12)\n",
    "g = torch.from_numpy(g_np)\n",
    "print(g)\n",
    "\n",
    "# More distributions and ways of initializing tensors here \n",
    "# http://pytorch.org/docs/master/torch.html#random-sampling\n",
    "# http://pytorch.org/docs/master/torch.html#tensors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We wrap our PyTorch Tensors in Variable objects; a Variable represents a node in a computational graph. If x is a Variable then x.data is a Tensor, and x.grad is another Variable holding the gradient of x with respect to some scalar value. PyTorch Variables have the same API as PyTorch Tensors: (almost) any operation that you can perform on a Tensor also works on Variables; the difference is that using Variables defines a computational graph, allowing you to automatically compute gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Computational Graph "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](comp_graph_pytorch.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manual Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](comp_graph_pytorch2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_data = [1.0, 2.0, 3.0]\n",
    "y_data = [2.0, 4.0, 6.0]\n",
    "\n",
    "w = 1.0  # a random guess: random value\n",
    "\n",
    "# our model forward pass\n",
    "\n",
    "\n",
    "def forward(x):\n",
    "    return x * w\n",
    "\n",
    "\n",
    "# Loss function\n",
    "def loss(x, y):\n",
    "    y_pred = forward(x)\n",
    "    return (y_pred - y) * (y_pred - y)\n",
    "\n",
    "\n",
    "# compute gradient\n",
    "def gradient(x, y):  # d_loss/d_w\n",
    "    return 2 * x * (x * w - y)\n",
    "\n",
    "# Before training\n",
    "print(\"predict (before training)\",  4, forward(4))\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(10):\n",
    "    for x_val, y_val in zip(x_data, y_data):\n",
    "        grad = gradient(x_val, y_val)\n",
    "        w = w - 0.01 * grad\n",
    "        l = loss(x_val, y_val)\n",
    "    print('Loss after', epoch, 'epochs', l)\n",
    "    print('Value of w after', epoch,'epochs',w)\n",
    "\n",
    "# After training\n",
    "print(\"predict (after training)\",  \"4 hours\", forward(4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Automated Gradient Descent using Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "x_data = [1.0, 2.0, 3.0]\n",
    "y_data = [2.0, 4.0, 6.0]\n",
    "\n",
    "w = Variable(torch.Tensor([1.0]),  requires_grad=True)  # Any random value\n",
    "\n",
    "# our model forward pass\n",
    "\n",
    "\n",
    "def forward(x):\n",
    "    return x * w\n",
    "\n",
    "# Loss function\n",
    "\n",
    "\n",
    "def loss(x, y):\n",
    "    y_pred = forward(x)\n",
    "    return (y_pred - y) * (y_pred - y)\n",
    "\n",
    "# Before training\n",
    "print(\"predict (before training)\",  4, forward(4).data[0])\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(10):\n",
    "    for x_val, y_val in zip(x_data, y_data):\n",
    "        l = loss(x_val, y_val)\n",
    "        l.backward()\n",
    "        #update the weight variable\n",
    "        w.data = w.data - 0.01 * w.grad.data\n",
    "        # Manually zero the gradients after updating weights\n",
    "        w.grad.data.zero_()\n",
    "\n",
    "    new_loss = loss(x_val,y_val)\n",
    "    print('Loss after', epoch, 'epochs', new_loss.data[0])\n",
    "    print('Value of w after', epoch,'epochs',w.data)\n",
    "    \n",
    "# After training\n",
    "print(\"predict (after training)\",  4, forward(4).data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression Using Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](pytorch_rhythm.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "\"\"\"\n",
    "Data has to be in the form of a matrix with the first dimension being the sample number,\n",
    "second dimension being the observation and the third dimension being the features, we don't need the third dimesion in the \n",
    "example below\n",
    "\"\"\"\n",
    "x_data = Variable(torch.Tensor([[1.0], [2.0], [3.0]]))\n",
    "y_data = Variable(torch.Tensor([[2.0], [4.0], [6.0]]))\n",
    "\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        In the constructor we instantiate the nn.Linear module\n",
    "        First line has to be the call to the constructor\n",
    "        \"\"\"\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        \"\"\"\n",
    "        linear layer does take in two arguments for the instantiation which are the number of inputs\n",
    "        neurons and number of output neurons as shown above\n",
    "        \"\"\"\n",
    "        self.linear = nn.Linear(1, 1) \n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        In the forward function we accept a Variable of input data and we must return\n",
    "        a Variable of output data. We can use Modules defined in the constructor as\n",
    "        well as arbitrary operators on Variables.\n",
    "        \"\"\"\n",
    "        y_pred = self.linear(x)\n",
    "        return y_pred\n",
    "\n",
    "# our model\n",
    "model = Model()\n",
    "\n",
    "\n",
    "# Construct our loss function and an Optimizer. The call to model.parameters()\n",
    "# in the SGD constructor will contain the learnable parameters of the two\n",
    "# nn.Linear modules which are members of the model.\n",
    "criterion = nn.MSELoss(size_average=False)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(500):\n",
    "        # Forward pass: Compute predicted y by passing x to the model\n",
    "    y_pred = model(x_data)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = criterion(y_pred, y_data)\n",
    "    print(epoch, loss.data[0])\n",
    "\n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "# After training\n",
    "hour_var = Variable(torch.Tensor([[4.0]]))\n",
    "y_pred = model(hour_var)\n",
    "print(\"predict (after training)\",  4, model(hour_var).data[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression using Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "x_data = Variable(torch.Tensor([[1.0], [2.0], [3.0], [4.0]]))\n",
    "y_data = Variable(torch.Tensor([[0.], [0.], [1.], [1.]]))\n",
    "\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        In the constructor we instantiate nn.Linear module\n",
    "        \"\"\"\n",
    "        super(Model, self).__init__()\n",
    "        self.linear = nn.Linear(1, 1)  # One in and one out\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        In the forward function we accept a Variable of input data and we must return\n",
    "        a Variable of output data. Activation functions are applied to layers as shown below\n",
    "        \"\"\"\n",
    "        y_pred = F.sigmoid(self.linear(x))\n",
    "        return y_pred\n",
    "\n",
    "# our model\n",
    "model = Model()\n",
    "\n",
    "\n",
    "# Construct our loss function and an Optimizer. The call to model.parameters()\n",
    "# in the SGD constructor will contain the learnable parameters of the two\n",
    "# nn.Linear modules which are members of the model.\n",
    "criterion = torch.nn.BCELoss(size_average=True)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(1000):\n",
    "        # Forward pass: Compute predicted y by passing x to the model\n",
    "    y_pred = model(x_data)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = criterion(y_pred, y_data)\n",
    "    print(epoch, loss.data[0])\n",
    "\n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# After training\n",
    "hour_var = Variable(torch.Tensor([[1.0]]))\n",
    "print(\"predict 1 hour \", 1.0, model(hour_var).data[0][0] > 0.5)\n",
    "hour_var = Variable(torch.Tensor([[7.0]]))\n",
    "print(\"predict 7 hours\", 7.0, model(hour_var).data[0][0] > 0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple DNN using pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Torch NN module\n",
    "Neural networks can be constructed using the torch.nn module.\n",
    "Provides pretty much all neural network related functionalities such as :\n",
    "Linear layers - nn.Linear, nn.Bilinear\n",
    "Convolution Layers - nn.Conv1d, nn.Conv2d, nn.Conv3d, nn.ConvTranspose2d\n",
    "Nonlinearities - nn.Sigmoid, nn.Tanh, nn.ReLU, nn.LeakyReLU\n",
    "Pooling Layers - nn.MaxPool1d, nn.AveragePool2d\n",
    "Recurrent Networks - nn.LSTM, nn.GRU\n",
    "Normalization - nn.BatchNorm2d\n",
    "Dropout - nn.Dropout, nn.Dropout2d\n",
    "Embedding - nn.Embedding\n",
    "Loss Functions - nn.MSELoss, nn.CrossEntropyLoss, nn.NLLLoss\n",
    "Instances of these classes will have an __call__ function built-in that can be used to run an input through the layer.\n",
    "\n",
    "More Details here http://pytorch.org/docs/0.3.0/nn.html?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Linear Layers\n",
    "\n",
    "x = Variable(torch.randn(32, 10))\n",
    "y = Variable(torch.randn(32, 30))\n",
    "\n",
    "sigmoid = nn.Sigmoid()\n",
    "\n",
    "# y = Wx + b\n",
    "linear = nn.Linear(in_features=10, out_features=20, bias=True)\n",
    "output_linear = linear(x)\n",
    "print('Linear output size : ', output_linear.size())\n",
    "\n",
    "# y = x1*W*x2 + b \n",
    "bilinear = nn.Bilinear(in1_features=10, in2_features=30, out_features=50, bias=True)\n",
    "output_bilinear = bilinear(x, y)\n",
    "print('Bilinear output size : ', output_bilinear.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convolutional layers\n",
    "\n",
    "x = Variable(torch.randn(10, 3, 28, 28))\n",
    "\n",
    "conv = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=(3, 3), stride=1, padding=1, bias=True)\n",
    "bn = nn.BatchNorm2d(num_features=32)\n",
    "pool = nn.MaxPool2d(kernel_size=(2, 2), stride=2)\n",
    "\n",
    "output_conv = bn(conv(x))\n",
    "outpout_pool = pool(conv(x))\n",
    "\n",
    "print('Conv output size : ', output_conv.size())\n",
    "print('Pool output size : ', outpout_pool.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Recurrent, Embedding & Dropout Layers\n",
    "\n",
    "inputs = [[1, 2, 3], [1, 0, 4], [1, 2, 4], [1, 4, 0], [1, 3, 3]]\n",
    "x = Variable(torch.LongTensor(inputs))\n",
    "\n",
    "embedding = nn.Embedding(num_embeddings=5, embedding_dim=20, padding_idx=1)\n",
    "drop = nn.Dropout(p=0.5)\n",
    "gru = nn.GRU(input_size=20, hidden_size=50, num_layers=2, batch_first=True, bidirectional=True, dropout=0.3)\n",
    "\n",
    "emb = drop(embedding(x))\n",
    "gru_h, gru_h_t = gru(emb)\n",
    "\n",
    "print('Embedding size : ', emb.size())\n",
    "print('GRU hidden states size : ', gru_h.size())\n",
    "print('GRU last hidden state size : ', gru_h_t.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Torch.nn.functional\n",
    "\n",
    "Using the above classes requires defining an instance of the class and then running inputs through the instance.\n",
    "\n",
    "The functional API provides users a way to use these classes in a functional way. Such as\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "Linear layers - F.linear(input=x, weight=W, bias=b)\n",
    "\n",
    "Convolution Layers - F.conv2d(input=x, weight=W, bias=b, stride=1, padding=0, dilation=1, groups=1)\n",
    "\n",
    "Nonlinearities - F.sigmoid(x), F.tanh(x), F.relu(x), F.softmax(x)\n",
    "\n",
    "Dropout - F.dropout(x, p=0.5, training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = Variable(torch.randn(10, 3, 28, 28))\n",
    "filters = Variable(torch.randn(32, 3, 3, 3))\n",
    "conv_out = F.relu(F.dropout(F.conv2d(input=x, weight=filters, padding=1), p=0.5, training=True))\n",
    "\n",
    "print('Conv output size : ', conv_out.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Torch.nn.init\n",
    "\n",
    "Provides a set of functions for standard weight initialization techniques\n",
    "\n",
    "import torch.nn.init as init\n",
    "\n",
    "Calculate the gain of a layer based on the activation function - init.calculate_gain('sigmoid')\n",
    "\n",
    "Uniform init - init.uniform(tensor, low, high)\n",
    "\n",
    "Xavier uniform - init.xavier_uniform(tensor, gain=init.calculate_gain('sigmoid'))\n",
    "\n",
    "Xavier normal - init.xavier_normal(tensor, gain=init.calculate_gain('tanh'))\n",
    "\n",
    "Orthogonal - init.orthogonal(tensor, gain=init.calculate_gain('tanh'))\n",
    "\n",
    "Kaiming normal - init.kaiming_normal(tensor, mode='fan_in')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv_layer = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=(3, 3), padding=1)\n",
    "for k,v in conv_layer.named_parameters():\n",
    "    if k == 'weight':\n",
    "        init.kaiming_normal(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Torch.optim\n",
    "\n",
    "Provides implementations of standard stochastic optimization techniques\n",
    "\n",
    "1. SGD - optim.SGD([W1, W2], lr=0.01, momentum=0.9, dampening=0, weight_decay=1e-2, nesterov=True)\n",
    "2. Adam - optim.Adam([W1, W2], lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0)\n",
    "\n",
    "0ptim.lr_scheduler\n",
    "\n",
    "1. optim.lr_scheduler.MultiStepLR(optimizer, milestones=[30,80], gamma=0.1)\n",
    "2. optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, verbose=True, threshold=1e-04, threshold_mode='rel', min_lr=1e-05, eps=1e-08)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create random dataset using sklearn\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "batch_size = 100\n",
    "\n",
    "# MNIST Dataset \n",
    "train_dataset = dsets.MNIST(root='./data', \n",
    "                            train=True, \n",
    "                            transform=transforms.ToTensor(),  \n",
    "                            download=True)\n",
    "\n",
    "test_dataset = dsets.MNIST(root='./data', \n",
    "                           train=False, \n",
    "                           transform=transforms.ToTensor())\n",
    "\n",
    "# Data Loader (Input Pipeline)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (l1): Linear(in_features=784, out_features=200)\n",
      "  (l2): Linear(in_features=200, out_features=100)\n",
      "  (l3): Linear(in_features=100, out_features=10)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 5x5 square convolution\n",
    "        # kernel\n",
    "        self.l1 = nn.Linear(in_features=784, out_features=200, bias=True)\n",
    "        self.l2 = nn.Linear(in_features=200, out_features=100, bias=True)\n",
    "        self.l3 = nn.Linear(in_features=100, out_features=10, bias=True)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.relu(self.l1(x))\n",
    "        x = F.relu(self.l2(x))\n",
    "        x = F.softmax(self.l3(x))\n",
    "        return x\n",
    "\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()  \n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.01)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\khatwd2\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:23: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Step [100/600], Loss: 1.7069\n",
      "Epoch [1/20], Step [200/600], Loss: 1.6157\n",
      "Epoch [1/20], Step [300/600], Loss: 1.6488\n",
      "Epoch [1/20], Step [400/600], Loss: 1.5283\n",
      "Epoch [1/20], Step [500/600], Loss: 1.5210\n",
      "Epoch [1/20], Step [600/600], Loss: 1.5696\n",
      "Epoch [2/20], Step [100/600], Loss: 1.5634\n",
      "Epoch [2/20], Step [200/600], Loss: 1.5975\n",
      "Epoch [2/20], Step [300/600], Loss: 1.5766\n",
      "Epoch [2/20], Step [400/600], Loss: 1.5510\n",
      "Epoch [2/20], Step [500/600], Loss: 1.5812\n",
      "Epoch [2/20], Step [600/600], Loss: 1.5479\n",
      "Epoch [3/20], Step [100/600], Loss: 1.5612\n",
      "Epoch [3/20], Step [200/600], Loss: 1.5812\n",
      "Epoch [3/20], Step [300/600], Loss: 1.5511\n",
      "Epoch [3/20], Step [400/600], Loss: 1.5310\n",
      "Epoch [3/20], Step [500/600], Loss: 1.5614\n",
      "Epoch [3/20], Step [600/600], Loss: 1.6111\n",
      "Epoch [4/20], Step [100/600], Loss: 1.5991\n",
      "Epoch [4/20], Step [200/600], Loss: 1.6010\n",
      "Epoch [4/20], Step [300/600], Loss: 1.5714\n",
      "Epoch [4/20], Step [400/600], Loss: 1.6512\n",
      "Epoch [4/20], Step [500/600], Loss: 1.5411\n",
      "Epoch [4/20], Step [600/600], Loss: 1.5808\n",
      "Epoch [5/20], Step [100/600], Loss: 1.6212\n",
      "Epoch [5/20], Step [200/600], Loss: 1.5912\n",
      "Epoch [5/20], Step [300/600], Loss: 1.5809\n",
      "Epoch [5/20], Step [400/600], Loss: 1.5212\n",
      "Epoch [5/20], Step [500/600], Loss: 1.5412\n",
      "Epoch [5/20], Step [600/600], Loss: 1.6403\n",
      "Epoch [6/20], Step [100/600], Loss: 1.5912\n",
      "Epoch [6/20], Step [200/600], Loss: 1.6312\n",
      "Epoch [6/20], Step [300/600], Loss: 1.5812\n",
      "Epoch [6/20], Step [400/600], Loss: 1.7011\n",
      "Epoch [6/20], Step [500/600], Loss: 1.5712\n",
      "Epoch [6/20], Step [600/600], Loss: 1.6512\n",
      "Epoch [7/20], Step [100/600], Loss: 1.7812\n",
      "Epoch [7/20], Step [200/600], Loss: 1.7372\n",
      "Epoch [7/20], Step [300/600], Loss: 1.6711\n",
      "Epoch [7/20], Step [400/600], Loss: 1.6412\n",
      "Epoch [7/20], Step [500/600], Loss: 1.6712\n",
      "Epoch [7/20], Step [600/600], Loss: 1.6252\n",
      "Epoch [8/20], Step [100/600], Loss: 1.6012\n",
      "Epoch [8/20], Step [200/600], Loss: 1.5112\n",
      "Epoch [8/20], Step [300/600], Loss: 1.5575\n",
      "Epoch [8/20], Step [400/600], Loss: 1.6012\n",
      "Epoch [8/20], Step [500/600], Loss: 1.6012\n",
      "Epoch [8/20], Step [600/600], Loss: 1.6512\n",
      "Epoch [9/20], Step [100/600], Loss: 1.6112\n",
      "Epoch [9/20], Step [200/600], Loss: 1.5612\n",
      "Epoch [9/20], Step [300/600], Loss: 1.5711\n",
      "Epoch [9/20], Step [400/600], Loss: 1.5912\n",
      "Epoch [9/20], Step [500/600], Loss: 1.6712\n",
      "Epoch [9/20], Step [600/600], Loss: 1.6112\n",
      "Epoch [10/20], Step [100/600], Loss: 1.5712\n",
      "Epoch [10/20], Step [200/600], Loss: 1.5712\n",
      "Epoch [10/20], Step [300/600], Loss: 1.6112\n",
      "Epoch [10/20], Step [400/600], Loss: 1.5812\n",
      "Epoch [10/20], Step [500/600], Loss: 1.5912\n",
      "Epoch [10/20], Step [600/600], Loss: 1.6600\n",
      "Epoch [11/20], Step [100/600], Loss: 1.5812\n",
      "Epoch [11/20], Step [200/600], Loss: 1.7812\n",
      "Epoch [11/20], Step [300/600], Loss: 1.6312\n",
      "Epoch [11/20], Step [400/600], Loss: 1.6912\n",
      "Epoch [11/20], Step [500/600], Loss: 1.5712\n",
      "Epoch [11/20], Step [600/600], Loss: 1.5812\n",
      "Epoch [12/20], Step [100/600], Loss: 1.5612\n",
      "Epoch [12/20], Step [200/600], Loss: 1.5812\n",
      "Epoch [12/20], Step [300/600], Loss: 1.6212\n",
      "Epoch [12/20], Step [400/600], Loss: 1.5312\n",
      "Epoch [12/20], Step [500/600], Loss: 1.5612\n",
      "Epoch [12/20], Step [600/600], Loss: 1.5212\n",
      "Epoch [13/20], Step [100/600], Loss: 1.5712\n",
      "Epoch [13/20], Step [200/600], Loss: 1.6312\n",
      "Epoch [13/20], Step [300/600], Loss: 1.6712\n",
      "Epoch [13/20], Step [400/600], Loss: 1.6812\n",
      "Epoch [13/20], Step [500/600], Loss: 1.6111\n",
      "Epoch [13/20], Step [600/600], Loss: 1.6512\n",
      "Epoch [14/20], Step [100/600], Loss: 1.6112\n",
      "Epoch [14/20], Step [200/600], Loss: 1.7013\n",
      "Epoch [14/20], Step [300/600], Loss: 1.6912\n",
      "Epoch [14/20], Step [400/600], Loss: 1.5712\n",
      "Epoch [14/20], Step [500/600], Loss: 1.5512\n",
      "Epoch [14/20], Step [600/600], Loss: 1.6312\n",
      "Epoch [15/20], Step [100/600], Loss: 1.5712\n",
      "Epoch [15/20], Step [200/600], Loss: 1.6012\n",
      "Epoch [15/20], Step [300/600], Loss: 1.6112\n",
      "Epoch [15/20], Step [400/600], Loss: 1.7012\n",
      "Epoch [15/20], Step [500/600], Loss: 1.7308\n",
      "Epoch [15/20], Step [600/600], Loss: 1.6212\n",
      "Epoch [16/20], Step [100/600], Loss: 1.6512\n",
      "Epoch [16/20], Step [200/600], Loss: 1.6012\n",
      "Epoch [16/20], Step [300/600], Loss: 1.7312\n",
      "Epoch [16/20], Step [400/600], Loss: 1.6212\n",
      "Epoch [16/20], Step [500/600], Loss: 1.6512\n",
      "Epoch [16/20], Step [600/600], Loss: 1.6812\n",
      "Epoch [17/20], Step [100/600], Loss: 1.5912\n",
      "Epoch [17/20], Step [200/600], Loss: 1.6712\n",
      "Epoch [17/20], Step [300/600], Loss: 1.7012\n",
      "Epoch [17/20], Step [400/600], Loss: 1.5912\n",
      "Epoch [17/20], Step [500/600], Loss: 1.6312\n",
      "Epoch [17/20], Step [600/600], Loss: 1.6912\n",
      "Epoch [18/20], Step [100/600], Loss: 1.5712\n",
      "Epoch [18/20], Step [200/600], Loss: 1.5512\n",
      "Epoch [18/20], Step [300/600], Loss: 1.6412\n",
      "Epoch [18/20], Step [400/600], Loss: 1.6512\n",
      "Epoch [18/20], Step [500/600], Loss: 1.6912\n",
      "Epoch [18/20], Step [600/600], Loss: 1.6112\n",
      "Epoch [19/20], Step [100/600], Loss: 1.6012\n",
      "Epoch [19/20], Step [200/600], Loss: 1.6212\n",
      "Epoch [19/20], Step [300/600], Loss: 1.6012\n",
      "Epoch [19/20], Step [400/600], Loss: 1.6912\n",
      "Epoch [19/20], Step [500/600], Loss: 1.5912\n",
      "Epoch [19/20], Step [600/600], Loss: 1.6312\n",
      "Epoch [20/20], Step [100/600], Loss: 1.6612\n",
      "Epoch [20/20], Step [200/600], Loss: 1.6112\n",
      "Epoch [20/20], Step [300/600], Loss: 1.6512\n",
      "Epoch [20/20], Step [400/600], Loss: 1.6812\n",
      "Epoch [20/20], Step [500/600], Loss: 1.7212\n",
      "Epoch [20/20], Step [600/600], Loss: 1.7312\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 20\n",
    "# Train the Model\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):  \n",
    "        # Convert torch tensor to Variable\n",
    "        images = Variable(images.view(-1, 28*28))\n",
    "        labels = Variable(labels)\n",
    "        \n",
    "        # Forward + Backward + Optimize\n",
    "        optimizer.zero_grad()  # zero the gradient buffer\n",
    "        outputs = net(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch [%d/%d], Step [%d/%d], Loss: %.4f' \n",
    "                   %(epoch+1, num_epochs, i+1, len(train_dataset)//batch_size, loss.data[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\khatwd2\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:23: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 70 %\n"
     ]
    }
   ],
   "source": [
    "# Test the Model\n",
    "correct = 0\n",
    "total = 0\n",
    "for images, labels in test_loader:\n",
    "    images = Variable(images.view(-1, 28*28))\n",
    "    outputs = net(images)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save the Model\n",
    "torch.save(net.state_dict(), 'model.pkl')\n",
    "# Load the model\n",
    "the_model = Net()\n",
    "the_model.load_state_dict(torch.load('model.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\khatwd2\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:23: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 70 %\n"
     ]
    }
   ],
   "source": [
    "# Test the Model\n",
    "correct = 0\n",
    "total = 0\n",
    "for images, labels in test_loader:\n",
    "    images = Variable(images.view(-1, 28*28))\n",
    "    outputs = the_model(images)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (100 * correct / total))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
